{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping News with Selenium\n",
    "\n",
    "Kode untuk melakukan scraping pada website `detik`, `republika`, dan `kompas`. Untuk setiap website perlu dikumpulkan link artikelnya terlebih dahulu. Untuk `detik` sudah disertai dengan scraping link berita berdasarkan keyword tertentu. Tetapi untuk `republika` dan `kompas` belum bisa dilakukan karena hasil pencarian keyword untuk website tersebut dilakukan oleh [Programmable Search Engine - Google](https://programmablesearchengine.google.com/about/) dan sulit untuk membuat scrapernya\n",
    "\n",
    "Scraping dilakukan menggunakan [Selenium](https://www.selenium.dev/) sehingga perlu dilakukan configurasi terlebih dahulu yaitu install library `pip install selenium`, kemudian download [webdriver Chrome](https://chromedriver.chromium.org/downloads) dan atur `PATH` dari webdriver.\n",
    "\n",
    "Hasil scraping akan disimpan format `json` sebagai berikut\n",
    "\n",
    "```\n",
    "{\n",
    "    \"website\" : nama_website,\n",
    "    \"keyword\" : keyword_pencarian,\n",
    "    \"scraping_start\" : waktu_scraping_dimulai,\n",
    "    \"scraping_end\" : waktu_scraping_berakhir,\n",
    "    \"article\" : {\n",
    "        \"count\" : jumlah_artikel,\n",
    "        \"data\" : [\n",
    "            {\n",
    "                \"title\" : judul_artikel,\n",
    "                \"url\" : url_artikel,\n",
    "                \"published_at\" : waktu_publish_artikel,\n",
    "                \"full_content\" : isi_artikel,\n",
    "                \"paragraph\" : {\n",
    "                    \"count\" : jumlah_paragraf_dalam_artikel,\n",
    "                    \"data\" : {\n",
    "                        \"paragraf_1\" : paragraf_ke_1,\n",
    "                        ...\n",
    "                        \"paragtaf_n\" : paragraf_ke_n\n",
    "                    }\n",
    "                }\n",
    "            }, \n",
    "            ....\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Class\n",
    "\n",
    "Merupakan SuperClass yang nantinya akan diextend oleh class lainnya. Class ini berisi method-method yang secara umum bisa diterapkan diseluruh website berita. Jika nantinya ada yang berbeda maka akan di override pada subclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraping():\n",
    "    def __init__(self, container_article, website, keyword=None):\n",
    "        self.all_berita = []\n",
    "        self.driver =  None\n",
    "        self.container_article = container_article\n",
    "        self.website = website\n",
    "        self.keyword = keyword\n",
    "    \n",
    "    def get(self, url):\n",
    "        \"\"\"Membuat request dengan metode GET ke URL tertentu\"\"\"\n",
    "        try: self.driver.get(url)\n",
    "        except:pass\n",
    "        \n",
    "    def split_paragraph(self, p_elements):\n",
    "        \"\"\"Membaca seluruh paragraf disatu halaman republika\"\"\"\n",
    "        for p in p_elements:\n",
    "            text = p.text.strip()\n",
    "            if text:\n",
    "                self._paragraphs[f\"paragraf_{self._paragraphs_count}\"] = text\n",
    "                self._paragraphs_count += 1\n",
    "                \n",
    "    def get_p_elements(self):\n",
    "        \"\"\"Mencari semua tag <p> yang merupakan bagian dari artikel\"\"\"\n",
    "        return self.driver.find_elements(by=By.CSS_SELECTOR, value=f\".{self.container_article} p\")\n",
    "    \n",
    "    \n",
    "    def _init_paragraph(self):\n",
    "        \"\"\"Inisialisasi jumlah paragraf dalam 1 artikel\"\"\"\n",
    "        self._paragraphs = {}\n",
    "        self._paragraphs_count = 1\n",
    "    \n",
    "        \n",
    "    def _scraping(self, list_link, verbose=0, time_out=10, callback=None):\n",
    "        \"\"\"\n",
    "        Lakukan scarping berdasarkan url yang ada di variabel\n",
    "        list_link. Verbose selain 1 tidak akan melakukan print\n",
    "        url yang sedang di scaping di console. Time out berguna\n",
    "        untuk mengatur waktu loading maksmal dari website dalam\n",
    "        satuan detik. Variabel list_link harus bertipe list python\n",
    "        \"\"\"\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.set_page_load_timeout(time_out)\n",
    "        \n",
    "        for link in list_link:\n",
    "            if verbose == 1:\n",
    "                print(link)\n",
    "            \n",
    "            try:\n",
    "                self.get(link)\n",
    "                \n",
    "                # method get_info_article harus didefinisikan oleh class\n",
    "                # yang mengextend class Scarping\n",
    "                published_date, title = self.get_info_article()\n",
    "                self._init_paragraph()\n",
    "                self.split_paragraph(self.get_p_elements())\n",
    "\n",
    "                # callback jika satu berita ditampilkan dalam beberapa halaman\n",
    "                if callback is not None: callback(link)\n",
    "\n",
    "                self.all_berita.append({\n",
    "                    'title' : title,\n",
    "                    'url' : link,\n",
    "                    'published_at' : published_date,\n",
    "                    'full_content' : \" \".join([self._paragraphs[p] for p in self._paragraphs.keys()]),\n",
    "                    \"paragraph\" : {\n",
    "                        \"count\" : self._paragraphs_count - 1,\n",
    "                        \"data\" : self._paragraphs\n",
    "                    }\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"{link}. {str(e)}\")\n",
    "                \n",
    "        self.driver.close()\n",
    "        \n",
    "    def save_data(self, folder):\n",
    "        \"\"\"Save data to file json\"\"\"\n",
    "        data = {\n",
    "            \"website\" : self.website,\n",
    "            \"keyword\" : self.keyword,\n",
    "            \"scraping_start\" : self.scraping_start,\n",
    "            \"scraping_end\" : self.scraping_end,\n",
    "            \"article\" : {\n",
    "                \"count\" : len(self.all_berita),\n",
    "                \"data\" : self.all_berita\n",
    "            }        \n",
    "        }\n",
    "\n",
    "        with open(f\"{folder}/{self.website_name}-{self.keyword}.json\", 'w') as fp:\n",
    "            json.dump(data, fp)\n",
    "    \n",
    "    def run(self, list_link, verbose=0, time_out=3, callback=None):\n",
    "        \"\"\" Method ini yang di overiding di subclass jika memang diperlukan\"\"\"\n",
    "        self.scraping_start = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self._scraping(list_link, verbose, time_out, callback)\n",
    "        self.scraping_end = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Republika Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapingRepublika(Scraping):\n",
    "    def __init__(self, keyword):\n",
    "        super().__init__(\"artikel\", \"https://republika.co.id/\", keyword)\n",
    "        self.website_name = \"republika\"\n",
    "    \n",
    "    def get_info_article(self):\n",
    "        \"\"\"Ambil informasi waktu publish dan judul\"\"\"\n",
    "        published_date = self.driver.find_element(by=By.CSS_SELECTOR, value='.wrap_detail_set .date_detail p').text\n",
    "        title = self.driver.find_element(by=By.CSS_SELECTOR, value='.wrap_detail_set h1').text\n",
    "        return published_date, title\n",
    "    \n",
    "    def has_pagination(self, link):\n",
    "        \"\"\"Ada beberapa artikel yang terpecah menjadi beberapa halaman\"\"\"\n",
    "        try:\n",
    "            self.driver.find_element(by=By.CLASS_NAME, value=\"pagination\")\n",
    "            part = 1\n",
    "            while True:\n",
    "                self.get(f\"{link}-part{part}\")\n",
    "                p_elements = self.get_p_elements()\n",
    "\n",
    "                if len(p_elements) == 0: break\n",
    "                self.split_paragraph(p_elements)\n",
    "                part += 1\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    def run(self, list_link, verbose=0, time_out=3):\n",
    "        \"\"\"Overriding karena memakai callback has pagination\"\"\"\n",
    "        self.scraping_start = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self._scraping(list_link, verbose, time_out, callback=self.has_pagination)\n",
    "        self.scraping_end = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kompas Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapingKompas(Scraping):\n",
    "    def __init__(self, keyword):\n",
    "        super().__init__(\"read__content\", \"https://www.kompas.com/\", keyword)\n",
    "        self.website_name = \"kompas\"\n",
    "    \n",
    "    \n",
    "    def get_info_article(self):\n",
    "        \"\"\"Ambil informasi waktu publish dan judul\"\"\"\n",
    "        published_date = self.driver.find_element(by=By.CSS_SELECTOR, value='.read__time').text.replace(\"Kompas.com - \", \"\")\n",
    "        title = self.driver.find_element(by=By.CSS_SELECTOR, value='.read__title').text\n",
    "        return published_date, title\n",
    "    \n",
    "        \n",
    "    def run(self, list_link, verbose=0, time_out=3):\n",
    "        \"\"\"Override method karena link perlu ditambahkan query string\n",
    "        page=all diakhir setiap link agar web menampilkan berita dalam\n",
    "        satu halaman full\"\"\"\n",
    "        \n",
    "        new_list_link = []\n",
    "        for link in list_link:\n",
    "            new_list_link.append(f\"{link}?page=all\")\n",
    "        \n",
    "        self.scraping_start = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self._scraping(new_list_link, verbose, time_out)\n",
    "        self.scraping_end = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detik Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapingDetik(Scraping):\n",
    "    def __init__(self, keyword):\n",
    "        super().__init__(\"detail__body-text\", \"http://detik.com/\", keyword)\n",
    "        self.website_name = \"detik\"\n",
    "                    \n",
    "    def get_info_article(self):\n",
    "        \"\"\"Ambil informasi waktu publish dan judul\"\"\"\n",
    "        published_date = self.driver.find_element(by=By.CSS_SELECTOR, value='.detail__date').text\n",
    "        title = self.driver.find_element(by=By.CSS_SELECTOR, value='h1.detail__title').text\n",
    "        return published_date, title\n",
    "    \n",
    "    def get_p_elements(self):\n",
    "        \"\"\"Mencari semua tag <p> dan div yang merupakan bagian dari artikel\"\"\"\n",
    "        p = self.driver.find_elements(by=By.CSS_SELECTOR, value=f\".{self.container_article} p\")\n",
    "        p += self.driver.find_elements(by=By.CSS_SELECTOR, value=f'.{self.container_article} div[style=\"text-align: left;\"]')\n",
    "        return p\n",
    "    \n",
    "    def scraping_title(self):\n",
    "        self.driver = webdriver.Chrome()\n",
    "        self.driver.get(self.website)\n",
    "        self.driver.find_element(by=By.CSS_SELECTOR, value='input[placeholder=\"Cari Berita\"]').send_keys(self.keyword, Keys.ENTER)\n",
    "\n",
    "        link_detik = []\n",
    "        while True:\n",
    "            a_elements = self.driver.find_elements(by=By.CSS_SELECTOR, value=\".list-berita article a\")\n",
    "            for a in a_elements:\n",
    "                link_detik.append(a.get_attribute(\"href\") + \"?single=1\")\n",
    "                \n",
    "            button = self.driver.find_elements(by=By.CSS_SELECTOR, value='.paging.text_center a')[-1]\n",
    "            if button.get_attribute(\"href\") != self.driver.current_url: button.click()\n",
    "            else: break\n",
    "                \n",
    "        self.driver.close()\n",
    "        return link_detik\n",
    "        \n",
    "        \n",
    "    def run(self, verbose=0, time_out=3):\n",
    "        \"\"\"\n",
    "        Overriding method karena perlu lakukan scraping title terlebih dahulu\n",
    "        \"\"\"\n",
    "        self.scraping_start = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        self.list_link = self.scraping_title()\n",
    "        self._scraping(self.list_link, verbose, time_out)\n",
    "        self.scraping_end = datetime.today().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_link_berita(path):\n",
    "    \"\"\"Membaca file link berita yang memiliki format\n",
    "    link1,link2,link3,....linkn\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        link_kompas = f.read().split(\",\")\n",
    "        return list(set(link_kompas)) # remove duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main\n",
    "\n",
    "Dicontohkan melakukan scraping data dengan keyword `Permendikbud No 30 Tahun 2021`. Untuk `republika` dan `kompas` pengumpulan linknya dilakukan secara manual dan disimpan dalam file `data/list-berita-republika.txt` dan `data/list-berita-kompas.txt`.\n",
    "\n",
    "Hasil scrapingnya akan disimpan dalam folder `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_republika = read_link_berita('data/list-berita-republika.txt')\n",
    "republika = ScrapingRepublika(\"Permendikbud No 30 Tahun 2021\")\n",
    "republika.run(link_republika, verbose=0, time_out=2)\n",
    "republika.save_data(\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_kompas = read_link_berita('data/list-berita-kompas.txt')\n",
    "kompas = ScrapingKompas(\"Permendikbud No 30 Tahun 2021\")\n",
    "kompas.run(link_kompas, verbose=0, time_out=2)\n",
    "kompas.save_data('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detik = ScrapingDetik(\"Permendikbud No 30 Tahun 2021\")\n",
    "detik.run(verbose=0, time_out=2)\n",
    "detik.save_data('data')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b37c7e2f1cbf0fcabc84e35824daec9dc55100997e5b7a146d1c3c450e4ba86b"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit ('selenium')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
